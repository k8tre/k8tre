{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K8TRE Documentation","text":"<p>AI Generated placeholder text - Not sanity checked!</p> <p>Welcome to the K8TRE documentation.</p>"},{"location":"#overview","title":"Overview","text":"<p>K8TRE is a project aimed at [provide brief description of your project here].</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Feature 1</li> <li>Feature 2</li> <li>Feature 3</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Getting Started guide to begin working with K8TRE.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>For a deep dive into the architecture, visit the Architecture section.</p>"},{"location":"architecture/","title":"K8TRE Architecture","text":""},{"location":"architecture/#how-the-k8tre-reference-implementation-meets-the-k8tre-specification","title":"How the K8TRE Reference Implementation meets the K8TRE Specification","text":""},{"location":"architecture/#byo-software","title":"BYO Software","text":""},{"location":"architecture/#container-runtimes","title":"Container Runtimes","text":"<p>The K8TRE Reference Implementation uses the default high- and low-level container runtimes in the EKS, AKS, K3S Kubernetes distributions. </p>"},{"location":"architecture/#databases","title":"Databases","text":"<p>The K8TRE Reference Implementation includes the CNPG operator and a default Postgres database. Applications can deploy their own Postgres databases in a consistent manner using the operator.</p>"},{"location":"architecture/#dns","title":"DNS","text":"<p>For in-cluster services, the Kubernetes default CoreDNS will be used, so clients can access services by servicename.namespace without a separate DNS server.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":""},{"location":"architecture/#load-balancers","title":"Load Balancers","text":""},{"location":"architecture/#networking","title":"Networking","text":""},{"location":"architecture/#secrets","title":"Secrets","text":""},{"location":"architecture/#storage","title":"Storage","text":""},{"location":"contributing/","title":"Contributing","text":"<p>AI Generated placeholder text - Not sanity checked!</p> <p>This page provides documentation on how to contribute to the K8TRE project.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>We welcome contributions from the community! Here's how you can help:</p>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a suggestion for improving the project:</p> <ol> <li>Check if the issue already exists in our issue tracker</li> <li>If not, create a new issue with a descriptive title and detailed description</li> <li>Include steps to reproduce, expected behavior, and actual behavior</li> </ol>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":"<p>Please refer to our CONTRIBUTING.md file in the repository root for detailed instructions on:</p> <ul> <li>Setting up your development environment</li> <li>Code style guidelines</li> <li>Pull request process</li> <li>Review process</li> </ul>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>Documentation improvements are just as valuable as code contributions!</p> <ol> <li>Fork the repository</li> <li>Make your changes to the documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Test your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing:</p> <ul> <li>Join our community chat</li> <li>Ask questions in the issues section</li> <li>Reach out to the maintainers</li> </ul> <p>Thank you for your interest in improving K8TRE!</p>"},{"location":"getting-started/","title":"Getting Started with K8TRE","text":"<p>AI Generated placeholder text - Not sanity checked!</p> <p>This guide will help you get up and running with K8TRE.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prerequisite 1</li> <li>Prerequisite 2</li> <li>Prerequisite 3</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Follow our Installation Guide for detailed instructions.</p>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<pre><code># Example command\nkubectl apply -f your-k8tre-config.yaml\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>After you've got the basics set up, you might want to:</p> <ol> <li>Configure your deployment</li> <li>Explore the Architecture</li> <li>Check the Reference documentation</li> </ol>"},{"location":"reference/","title":"References","text":""},{"location":"development/devcontainer/","title":"Using the DevContainer for K8TRE Development","text":"<p>K8TRE now provides a Visual Studio Code DevContainer configuration to simplify setup of development environments. This guide explains how to use this feature.</p>"},{"location":"development/devcontainer/#what-is-a-devcontainer","title":"What is a DevContainer?","text":"<p>A Development Container (DevContainer) defines a consistent, reproducible development environment with all tools and dependencies pre-configured. This eliminates \"works on my machine\" issues and speeds up onboarding for new contributors.</p>"},{"location":"development/devcontainer/#prerequisites","title":"Prerequisites","text":"<p>To use the DevContainer, you'll need:</p> <ol> <li>Visual Studio Code installed on your computer</li> <li>Docker Desktop (or compatible container runtime)</li> <li>Visual Studio Code Remote - Containers extension</li> </ol>"},{"location":"development/devcontainer/#using-the-devcontainer","title":"Using the DevContainer","text":"<ol> <li> <p>Clone the K8TRE repository:    <pre><code>git clone https://github.com/k8tre/k8tre.git\ncd k8tre\n</code></pre></p> </li> <li> <p>Open the folder in Visual Studio Code:    <pre><code>code .\n</code></pre></p> </li> <li> <p>When prompted, click \"Reopen in Container\", or use the Command Palette (F1) and select \"Remote-Containers: Reopen in Container\".</p> </li> <li> <p>VS Code will build the DevContainer (this may take several minutes on the first run).</p> </li> <li> <p>Once the container is ready, you'll have access to:</p> </li> <li>All required CLI tools (kubectl, kustomize, helm, argocd, etc.)</li> <li>Proper Python environment with dependencies</li> <li>Pre-configured Kubernetes extensions</li> <li>K3s for local development</li> </ol>"},{"location":"development/devcontainer/#starting-development","title":"Starting Development","text":"<p>Once inside the container:</p> <ol> <li>The container is already configured with:</li> <li>K3s running for local Kubernetes development</li> <li>Kubectl configured with the proper kubeconfig</li> <li> <p>ArgoCD installed and ready to use</p> </li> <li> <p>Apply the K8TRE resources:    <pre><code>kubectl apply -f local/root-app-of-apps.yaml\n</code></pre></p> </li> <li> <p>Access the ArgoCD UI:</p> </li> <li>Set up port forwarding to access the ArgoCD UI:      <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre></li> <li>Open https://localhost:8080 (or more likely https://localhost:8081 - check PORTS tab in VS Code) in your browser</li> <li>Get the initial admin password. This is displayed at startup or run the following command.      <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre></li> <li> <p>Log in with username: admin and the password retrieved above</p> </li> <li> <p>Follow the regular K8TRE development workflow</p> </li> </ol>"},{"location":"development/devcontainer/#github-codespaces","title":"GitHub Codespaces","text":"<p>The same DevContainer configuration works with GitHub Codespaces. To use it:</p> <ol> <li>Go to the K8TRE GitHub repository</li> <li>Click the \"Code\" button</li> <li>Select the \"Codespaces\" tab</li> <li>Click \"Create codespace on main\"</li> </ol> <p>This will create a cloud-based development environment with the same configuration.</p>"},{"location":"development/devcontainer/#customizing-the-devcontainer","title":"Customizing the DevContainer","text":"<p>If you need to customize your development environment, you can modify:</p> <ul> <li><code>.devcontainer/devcontainer.json</code> - Main configuration file</li> <li><code>.devcontainer/post-create.sh</code> - Script that runs after container creation</li> </ul> <p>See the DevContainer README for more details.</p>"},{"location":"development/introduction/","title":"Introduction to Developing K8TRE","text":""},{"location":"development/k3s-dev/","title":"Setting up K3s all-in-one Development Environment","text":"<p>This documentation guides you through creating an all-in-one development environment using a single K3s cluster (ArgoCD is deployment in the same cluster as a K8TRE dev deployment).</p> <p>These instructions are also used for automatically testing K8TRE in GitHub actions. For more hands-on instructions with explanations see k3s.md.</p>"},{"location":"development/k3s-dev/#prerequisites","title":"Prerequisites","text":"<p>This assumes you already have a Ubuntu 24.04 virtual machine.</p>"},{"location":"development/k3s-dev/#configure-and-install-k3s","title":"Configure and install K3s","text":"<p>Create a K3s configuration file, then install K3S. Although most commands can be passed to the command line installer it is more convenient to define them in a K3S configuration file. We'll be installing Cilium, so disable the default Flannel CNI.</p> <pre><code>sudo mkdir -p /etc/rancher/k3s\nsudo tee /etc/rancher/k3s/config.yaml &lt;&lt; EOF\nnode-name: k8tre-dev\ntls-san:\n  - k8tre-dev\ncluster-init: true\n\n# Custom CNI: https://docs.k3s.io/networking/basic-network-options#custom-cni\nflannel-backend: none\ndisable-network-policy: true\ndisable:\n  - traefik\n  - servicelb\nEOF\n\ncurl -sfSL https://get.k3s.io | INSTALL_K3S_VERSION=v1.32.4+k3s1 sh -\n</code></pre> <p>Setup Kubeconfig file</p> <pre><code>mkdir -p ~/.kube\nsudo cat /etc/rancher/k3s/k3s.yaml &gt; ~/.kube/config\n</code></pre>"},{"location":"development/k3s-dev/#setup-cilium-cni","title":"Setup Cilium CNI","text":"<p>https://docs.cilium.io/en/stable/installation/k3s/</p> <pre><code>CILIUM_VERSION=1.17.5\nCILIUM_CLI_VERSION=v0.18.4\nK3S_POD_CIDR=10.42.0.0/16\n\ncurl -sfSL https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-amd64.tar.gz | sudo tar -zxvf - -C /usr/local/bin/\ncilium install --version $CILIUM_VERSION --set=ipam.operator.clusterPoolIPv4PodCIDRList=\"$K3S_POD_CIDR\" --set cni.chainingMode=portmap\n</code></pre> <p>Install portmap plugin for hostport support</p> <pre><code>sudo mkdir -p /opt/cni/bin/\ncurl -sfSL https://github.com/containernetworking/plugins/releases/download/v1.7.1/cni-plugins-linux-amd64-v1.7.1.tgz | sudo tar -zxvf - -C /opt/cni/bin/ ./portmap\n</code></pre> <p>Wait for Cilium to be ready, and optionally check Cilium it's working. <pre><code>cilium status --wait\n# Uncomment if you need to verify cilium is working\n# cilium connectivity test\n</code></pre></p>"},{"location":"development/k3s-dev/#32-enable-required-add-ons","title":"3.2 Enable Required Add-ons","text":"<p>Enable hostpath-storage:</p> <pre><code># kubectl apply -f https://raw.githubusercontent.com/helm/charts/master/stable/hostpath-provisioner/hostpath-provisioner.yaml\n\n# Metallb is now installed by Argocd\n</code></pre>"},{"location":"development/k3s-dev/#setup-argocd","title":"Setup ArgoCD","text":"<pre><code>ARGOCD_VERSION=v3.0.6\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/install.yaml\nsleep 10\nkubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s\n\nsudo curl -sfSL https://github.com/argoproj/argo-cd/releases/download/$ARGOCD_VERSION/argocd-linux-amd64 -o /usr/local/bin/argocd\nsudo chmod a+x /usr/local/bin/argocd\n</code></pre> <p>Configure ArgoCD</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443 --address 0.0.0.0 &amp;\nsleep 1\n\n# Get the initial admin password\nARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\n\n# Login, disable certificate validation\nargocd login localhost:8080 --username=admin --password=\"$ARGOCD_PASSWORD\" --insecure\n</code></pre> <p>Mark the current cluster as the ArgoCD dev environment</p> <pre><code>argocd cluster set in-cluster \\\n  --label environment=dev \\\n  --label secret-store=kubernetes \\\n  --label vendor=k3s \\\n  --label skip-metallb=true\n\nargocd cluster get in-cluster\n</code></pre> <p>Configure ArgoCD for Kustomize Helm Overlays by applying a ConfigMap patch:</p> <pre><code>cat &lt;&lt; EOF &gt; argocd-cm-patch.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  kustomize.buildOptions: \"--enable-helm --load-restrictor LoadRestrictionsNone\"\nEOF\n\nkubectl apply -f argocd-cm-patch.yaml\n</code></pre> <p>Restart the ArgoCD repo server to apply changes:</p> <pre><code>kubectl rollout restart deployment argocd-repo-server -n argocd\n</code></pre> <p>List CRDs <pre><code>kubectl get crd\n</code></pre></p>"},{"location":"development/k3s-dev/#create-secrets-for-use-by-external-secrets-operator","title":"Create secrets for use by External Secrets Operator","text":"<pre><code>uv run ci/create-ci-secrets.py --context $(kubectl config current-context)\n</code></pre>"},{"location":"development/k3s-dev/#deploy-the-app-of-apps","title":"Deploy the app-of-apps","text":"<p>Edit the app-of-apps to point to you GitHub fork <code>$GITHUB_REPOSITORY</code> and branch/commit <code>$GITHUB_SHA</code></p> <pre><code>sed -i -e \"s%/k8tre/k8tre%/${GITHUB_REPOSITORY}%\" -e \"s%main%${GITHUB_SHA}%\" app_of_apps/root-app-of-apps.yaml\ngit diff\n</code></pre> <p>Deploy the app-of-apps <pre><code>kubectl apply -f app_of_apps/root-app-of-apps.yaml\n</code></pre></p> <p>Wait for app-of-apps to be created <pre><code>sleep 60\nkubectl -n argocd wait --for=jsonpath='{.status.health.status}'=Healthy application root-app-of-apps --timeout=300s\n</code></pre></p> <pre><code>kubectl get appprojects -A\nkubectl get applicationsets -A\nkubectl get applications -A\n</code></pre> <p>Wait for applications in app-of-apps to be created <pre><code>kubectl -n argocd wait --for=jsonpath='{.status.health.status}'=Healthy application --all --timeout=300s\n</code></pre></p> <pre><code>kubectl get deployment -A\nkubectl get daemonset -A\nkubectl get crd\n</code></pre>"},{"location":"development/k3s-dev/#setup-keycloak","title":"Setup Keycloak","text":"<pre><code># Get the initial admin password\nKEYCLOAK_PASSWORD=$(kubectl -nkeycloak get secret keycloak-admin-secret -o jsonpath='{.data.admin-password}' | base64 -d)\n# Wait a minute to ensure Keycloak is actually ready\nsleep 1m\n\nci/ci-setup-keycloak.py \\\n  --keycloak-admin=admin \\\n  --keycloak-password=\"$KEYCLOAK_PASSWORD\" \\\n  --verify=false\n</code></pre> <p>TODO: - Check all references to <code>k8tre/k8tre</code> and <code>main</code> are changed in all applications - Check number of applications is as expected - Check all applications are synchronised - Check deployments/daemonsets etc are healthy and ready - Playright test to login and launch workspace</p>"},{"location":"development/k3s/","title":"Setting up K3s Development Environment","text":"<p>This documentation guides you through creating a development environment using K3s. Alternative approaches include KinD, vCluster, and others.</p>"},{"location":"development/k3s/#overview","title":"Overview","text":"<p>This guide will help you set up two Ubuntu 24.04 VMs with K3s: - A management cluster with ArgoCD installed - A development cluster registered with ArgoCD for application deployment</p> <p>ArgoCD on the management cluster will deploy and manage applications on the development cluster.</p>"},{"location":"development/k3s/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu 24.04 on your local development machine</li> <li>kubectl (with kustomize)</li> <li>ArgoCD CLI</li> <li>Helm</li> <li>QEMU/KVM and Virtual Machine Manager</li> </ul>"},{"location":"development/k3s/#step-1-creating-virtual-machines","title":"Step 1: Creating Virtual Machines","text":""},{"location":"development/k3s/#11-create-two-vms","title":"1.1 Create Two VMs","text":"<p>Create two identical VMs (QEMU/KVM based) using Virtual Machine Manager:</p> <ol> <li>Open Virtual Machine Manager</li> <li>Click \"Create new virtual machine\"</li> <li>Select \"Local install media\" and choose Ubuntu 24.04 ISO</li> <li>Configure with at least 2 CPUs, 4GB RAM, and 20GB storage</li> <li>Complete the installation process for both VMs</li> </ol>"},{"location":"development/k3s/#12-set-hostname-and-hosts","title":"1.2 Set Hostname and Hosts","text":"<p>Name the VMs <code>vm-mgmt</code> and <code>vm-dev</code> respectively by editing these files:</p> <pre><code># On the management VM\nsudo nano /etc/hostname\n# Change to: vm-mgmt\n\nsudo nano /etc/hosts\n# Add: 127.0.1.1 vm-mgmt\n\nsudo hostnamectl set-hostname vm-mgmt\nsudo reboot\n\n# On the development VM\nsudo nano /etc/hostname\n# Change to: vm-dev\n\nsudo nano /etc/hosts\n# Add: 127.0.1.1 vm-dev\n\nsudo hostnamectl set-hostname vm-dev\nsudo reboot\n</code></pre>"},{"location":"development/k3s/#step-2-network-configuration","title":"Step 2: Network Configuration","text":""},{"location":"development/k3s/#21-create-a-static-network","title":"2.1 Create a Static Network","text":"<p>Create a new network in Virtual Machine Manager with static IP addresses:</p> <ol> <li>Open Virtual Machine Manager</li> <li>Go to Edit \u2192 Connection Details \u2192 Virtual Networks \u2192 +</li> <li>Or create the network XML file directly:</li> </ol> <pre><code>&lt;network connections=\"2\"&gt;\n  &lt;name&gt;static-network&lt;/name&gt;\n  &lt;uuid&gt;0f73f3fd-38f6-4e72-aa94-7984c2606054&lt;/uuid&gt;\n  &lt;forward mode=\"nat\"&gt;\n    &lt;nat&gt;\n      &lt;port start=\"1024\" end=\"65535\"/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name=\"virbr1\" stp=\"on\" delay=\"0\"/&gt;\n  &lt;mac address=\"52:54:00:4f:af:6c\"/&gt;\n  &lt;domain name=\"k8tre.internal\"/&gt;\n  &lt;ip address=\"192.168.123.1\" netmask=\"255.255.255.0\"&gt;\n    &lt;dhcp&gt;\n      &lt;range start=\"192.168.123.2\" end=\"192.168.123.254\"/&gt;\n      &lt;host mac=\"52:54:00:c8:e0:2f\" name=\"dev\" ip=\"192.168.123.62\"/&gt;\n      &lt;host mac=\"52:54:00:07:36:8a\" name=\"mgmt\" ip=\"192.168.123.52\"/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\n</code></pre> <p>Note: The IP address <code>192.168.123.1</code> refers to your host machine's IP on this virtual network.</p>"},{"location":"development/k3s/#22-attach-vms-to-network","title":"2.2 Attach VMs to Network","text":"<ol> <li>Edit each VM's configuration</li> <li>Change the network interface to the newly created static-network</li> <li>Make sure the MAC addresses match those specified in the network XML</li> <li>Restart the VMs so they get their new IP addresses</li> </ol>"},{"location":"development/k3s/#step-3-installing-and-configuring-k3s","title":"Step 3: Installing and Configuring K3s","text":""},{"location":"development/k3s/#31-install-k3s","title":"3.1 Install K3s","text":"<p>Install K3s on both VMs:</p> <p>Disable flannel and network policy as Cilium does this as part of k8tre. Diable traefik as k8tre uses nginx for ingress Disable servicelb as we will need metallb to provide an 'external loadbalancer'.</p> <p>However, note that metallb will not work on cloud providers.</p> <pre><code># On both VMs\ncurl -sfL https://get.k3s.io | sh - --flannel-backend=none --disable-network-policy --disable=traefik --disable=servicelb\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre>"},{"location":"development/k3s/#32-enable-required-add-ons","title":"3.2 Enable Required Add-ons","text":"<p>Enable MetalLB and hostpath-storage on both VMs:</p> <pre><code># On both VMs\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.15.2/config/manifests/metallb-native.yaml\nkubectl apply -f https://raw.githubusercontent.com/helm/charts/master/stable/hostpath-provisioner/hostpath-provisioner.yaml\n</code></pre> <p>See https://metallb.universe.tf/configuration/ and https://metallb.universe.tf/configuration/k3s/ for more details on setting up metallb on k3s.</p> <p>Metallb needs to be configured with an IP address pool which needs to be _advertised'.</p> <p>For instance, </p> <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: default-pool \n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.123.50-192.168.123.100\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: default-advertisement\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - default-pool\n</code></pre> <p>Create a yaml file with the above configuration and apply to your cluster. </p> <p>This is a temporary workaround while we integrate this into the ArgoCD workflow for k3s.</p>"},{"location":"development/k3s/#step-4-export-and-merge-kubeconfig","title":"Step 4: Export and Merge Kubeconfig","text":""},{"location":"development/k3s/#41-export-kubeconfig-from-vms","title":"4.1 Export Kubeconfig from VMs","text":"<pre><code># On the management VM\nk3s kubectl config view --raw &gt; ~/.kube/config\n\n# On the development VM\nk3s kubectl config view --raw &gt; ~/.kube/config\n</code></pre>"},{"location":"development/k3s/#42-copy-kubeconfig-files-to-host","title":"4.2 Copy Kubeconfig Files to Host","text":"<pre><code># From your host machine\nscp &lt;username&gt;@192.168.123.52:~/.kube/config ~/.kube/mgmt-config\nscp &lt;username&gt;@192.168.123.62:~/.kube/config ~/.kube/dev-config\n</code></pre>"},{"location":"development/k3s/#43-edit-and-merge-kubeconfig-files","title":"4.3 Edit and Merge Kubeconfig Files","text":"<p>Edit each kubeconfig file to rename clusters and users:</p> <pre><code># Edit mgmt-kubeconfig.yaml to change:\n# - cluster name to \"k3s-mgmt\" \n# - user name to \"admin-mgmt\"\n\n# Edit dev-kubeconfig.yaml to change:\n# - cluster name to \"k3s-dev\"\n# - user name to \"admin-dev\"\n</code></pre> <p>Then merge them:</p> <pre><code>KUBECONFIG=~/.kube/config:~/.kube/mgmt-config:~/.kube/dev-config kubectl config view --flatten &gt; ~/.kube/merged-config\nmv ~/.kube/merged-config ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre>"},{"location":"development/k3s/#step-5-setting-up-argocd","title":"Step 5: Setting Up ArgoCD","text":""},{"location":"development/k3s/#51-install-argocd-on-management-cluster","title":"5.1 Install ArgoCD on Management Cluster","text":"<pre><code># Switch to management cluster context\nkubectl config use-context k3s-mgmt\n\n# Create namespace\nkubectl create namespace argocd\n\n# Install ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Wait for all pods to be ready\nkubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s\n</code></pre>"},{"location":"development/k3s/#52-access-argocd-ui","title":"5.2 Access ArgoCD UI","text":"<pre><code># Port-forward ArgoCD server (run this in a separate terminal)\nkubectl port-forward svc/argocd-server -n argocd 8080:443 --address 0.0.0.0\n\n# Get the initial admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Visit https://localhost:8080 to access the ArgoCD UI.</p>"},{"location":"development/k3s/#53-configure-argocd-cli","title":"5.3 Configure ArgoCD CLI","text":"<pre><code># Login to ArgoCD from your host\nargocd login localhost:8080\n\n# Or if you've set up /etc/hosts entries:\nargocd login mgmt.k8tre.internal:8080\n</code></pre>"},{"location":"development/k3s/#54-register-development-cluster-with-argocd","title":"5.4 Register Development Cluster with ArgoCD","text":"<pre><code># Add the development cluster to ArgoCD\nargocd cluster add k3s-dev --name dev --label environment=dev\n</code></pre>"},{"location":"development/k3s/#55-configure-argocd-for-kustomize-helm-overlays","title":"5.5 Configure ArgoCD for Kustomize Helm Overlays","text":"<p>Create and apply a ConfigMap patch:</p> <pre><code>cat &lt;&lt; EOF &gt; argocd-cm-patch.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  kustomize.buildOptions: \"--enable-helm --load-restrictor LoadRestrictionsNone\"\nEOF\n\nkubectl apply -f argocd-cm-patch.yaml\n</code></pre> <p>Restart the ArgoCD repo server to apply changes:</p> <pre><code>kubectl rollout restart deployment argocd-repo-server -n argocd\n</code></pre> <p>Learn more about ArgoCD configuration options.</p>"},{"location":"development/k3s/#step-6-convenience-configurations","title":"Step 6: Convenience Configurations","text":""},{"location":"development/k3s/#61-context-switching-aliases","title":"6.1 Context-Switching Aliases","text":"<p>Add to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code># For easy context switching\nalias kdev='kubectl config use-context k3s-dev'\nalias kmgmt='kubectl config use-context k3s-mgmt'\n\n# Optionally add kubectl aliases\nalias k='kubectl'\nalias kgp='kubectl get pods'\nalias kgs='kubectl get services'\n</code></pre> <p>Then source your file: <code>source ~/.bashrc</code></p>"},{"location":"development/k3s/#62-host-file-entries","title":"6.2 Host File Entries","text":"<p>Add the following entries to <code>/etc/hosts</code> on your host machine:</p> <pre><code>sudo bash -c 'cat &lt;&lt; EOF &gt;&gt; /etc/hosts\n# VMs running K3s\n192.168.123.52 mgmt.k8tre.internal\n192.168.123.62 dev.k8tre.internal\nEOF'\n</code></pre> <p>Note: Be careful not to assign IP addresses that might conflict with other devices on your network.</p>"},{"location":"development/k3s/#step-7-verification","title":"Step 7: Verification","text":""},{"location":"development/k3s/#71-test-cluster-access","title":"7.1 Test Cluster Access","text":"<pre><code># Test management cluster\nkmgmt\nkubectl get nodes\n\n# Test development cluster\nkdev\nkubectl get nodes\n</code></pre>"},{"location":"development/k3s/#72-verify-argocd-setup","title":"7.2 Verify ArgoCD Setup","text":"<pre><code># Check that ArgoCD can communicate with both clusters\nargocd cluster list\n</code></pre>"},{"location":"development/k3s/#additional-resources","title":"Additional Resources","text":"<ul> <li>K3s Documentation</li> <li>Kubernetes Documentation</li> <li>ArgoCD Documentation</li> <li>Helm Documentation</li> <li>Kustomize Documentation</li> </ul>"},{"location":"development/k3s/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Check cluster status: <code>k3s kubectl get nodes</code></li> <li>View K3s logs: <code>journalctl -u k3s</code></li> <li>Check ArgoCD logs: <code>kubectl logs -n argocd deployment/argocd-server</code></li> <li>For networking issues, verify the VM network configuration.</li> </ol>"},{"location":"development/kind/","title":"Kind multi-cluster setup","text":"<p>DO NOT USE - Not tested yet</p> <p>https://gist.github.com/developer-guy/173347e71f92a61abbc017deb518b6cb</p>"},{"location":"development/kind/#create-management-cluster","title":"Create management cluster","text":"<pre><code>cat &lt;&lt;EOF | kind create cluster --name mgmt --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnetworking:\n  podSubnet: \"10.110.0.0/16\"\n  serviceSubnet: \"10.115.0.0/16\"\nnodes:\n- role: control-plane\n- role: worker\nEOF\n</code></pre>"},{"location":"development/kind/#create-dev-cluster","title":"Create dev cluster","text":"<pre><code>cat &lt;&lt;EOF | kind create cluster --name dev --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnetworking:\n  podSubnet: \"10.220.0.0/16\"\n  serviceSubnet: \"10.225.0.0/16\"\nnodes:\n- role: control-plane\n- role: worker\nEOF\n</code></pre> <pre><code>kubectl --context kind-mgmt get nodes -o=jsonpath='{range .items[*]}{\"ip route add \"}{.spec.podCIDR}{\" via \"}{.status.addresses[?(@.type==\"InternalIP\")].address}{\"\\n\"}{end}'\n</code></pre> <pre><code>kubectl --context kind-dev get nodes -o=jsonpath='{range .items[*]}{\"ip route add \"}{.spec.podCIDR}{\" via \"}{.status.addresses[?(@.type==\"InternalIP\")].address}{\"\\n\"}{end}'\n</code></pre>"},{"location":"development/microk8s/","title":"Setting up MicroK8s Development Environment","text":"<p>DO NOT USE MicroK8s</p> <p>Latest versions of Microk8s comes with Calico built-in and this conflicts with Cilium.   Use k3s instead and the other instructions in this document should still be applicable.   ToDo: Create a new page for documenting k3s setup. </p> <p>This documentation guides you through creating a development environment using MicroK8s.  Alternative approaches include K3s, KinD, vCluster, and others.</p>"},{"location":"development/microk8s/#overview","title":"Overview","text":"<p>This guide will help you set up two Ubuntu 24.04 VMs with MicroK8s: - A management cluster with ArgoCD installed - A development cluster registered with ArgoCD for application deployment</p> <p>ArgoCD on the management cluster will deploy and manage applications on the development cluster.</p>"},{"location":"development/microk8s/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu 24.04 on your local development machine</li> <li>kubectl (with kustomize)</li> <li>ArgoCD CLI</li> <li>Helm</li> <li>QEMU/KVM and Virtual Machine Manager</li> </ul>"},{"location":"development/microk8s/#step-1-creating-virtual-machines","title":"Step 1: Creating Virtual Machines","text":""},{"location":"development/microk8s/#11-create-two-vms","title":"1.1 Create Two VMs","text":"<p>Create two identical VMs (QEMU/KVM based) using Virtual Machine Manager:</p> <ol> <li>Open Virtual Machine Manager</li> <li>Click \"Create new virtual machine\"</li> <li>Select \"Local install media\" and choose Ubuntu 24.04 ISO</li> <li>Configure with at least 2 CPUs, 4GB RAM, and 20GB storage</li> <li>Complete the installation process for both VMs</li> </ol>"},{"location":"development/microk8s/#12-set-hostname-and-hosts","title":"1.2 Set Hostname and Hosts","text":"<p>Name the VMs <code>vm-mgmt</code> and <code>vm-dev</code> respectively by editing these files:</p> <pre><code># On the management VM\nsudo nano /etc/hostname\n# Change to: vm-mgmt\n\nsudo nano /etc/hosts\n# Add: 127.0.1.1 vm-mgmt\n\nsudo hostnamectl set-hostname vm-mgmt\nsudo reboot\n\n# On the development VM\nsudo nano /etc/hostname\n# Change to: vm-dev\n\nsudo nano /etc/hosts\n# Add: 127.0.1.1 vm-dev\n\nsudo hostnamectl set-hostname vm-dev\nsudo reboot\n</code></pre>"},{"location":"development/microk8s/#step-2-network-configuration","title":"Step 2: Network Configuration","text":""},{"location":"development/microk8s/#21-create-a-static-network","title":"2.1 Create a Static Network","text":"<p>Create a new network in Virtual Machine Manager with static IP addresses:</p> <ol> <li>Open Virtual Machine Manager</li> <li>Go to Edit \u2192 Connection Details \u2192 Virtual Networks \u2192 +</li> <li>Or create the network XML file directly:</li> </ol> <pre><code>&lt;network connections=\"2\"&gt;\n  &lt;name&gt;static-network&lt;/name&gt;\n  &lt;uuid&gt;0f73f3fd-38f6-4e72-aa94-7984c2606054&lt;/uuid&gt;\n  &lt;forward mode=\"nat\"&gt;\n    &lt;nat&gt;\n      &lt;port start=\"1024\" end=\"65535\"/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name=\"virbr1\" stp=\"on\" delay=\"0\"/&gt;\n  &lt;mac address=\"52:54:00:4f:af:6c\"/&gt;\n  &lt;domain name=\"k8tre.internal\"/&gt;\n  &lt;ip address=\"192.168.123.1\" netmask=\"255.255.255.0\"&gt;\n    &lt;dhcp&gt;\n      &lt;range start=\"192.168.123.2\" end=\"192.168.123.254\"/&gt;\n      &lt;host mac=\"52:54:00:c8:e0:2f\" name=\"dev\" ip=\"192.168.123.62\"/&gt;\n      &lt;host mac=\"52:54:00:07:36:8a\" name=\"mgmt\" ip=\"192.168.123.52\"/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\n</code></pre> <p>Note: The IP address <code>192.168.123.1</code> refers to your host machine's IP on this virtual network.</p>"},{"location":"development/microk8s/#22-attach-vms-to-network","title":"2.2 Attach VMs to Network","text":"<ol> <li>Edit each VM's configuration</li> <li>Change the network interface to the newly created static-network</li> <li>Make sure the MAC addresses match those specified in the network XML</li> <li>Restart the VMs so they get their new IP addresses</li> </ol>"},{"location":"development/microk8s/#step-3-installing-and-configuring-microk8s","title":"Step 3: Installing and Configuring MicroK8s","text":""},{"location":"development/microk8s/#31-install-microk8s","title":"3.1 Install MicroK8s","text":"<p>Install MicroK8s on both VMs:</p> <pre><code># On both VMs\nsudo snap install microk8s --classic --channel=1.28/stable\nsudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\nnewgrp microk8s\n</code></pre> <p>The alternative to the above is to enable microk8s during the VM creation process.</p>"},{"location":"development/microk8s/#32-enable-required-add-ons","title":"3.2 Enable Required Add-ons","text":"<p>Enable MetalLB and hostpath-storage on both VMs:</p> <pre><code># On both VMs\nmicrok8s enable dns\nmicrok8s enable hostpath-storage\n\n# For MetalLB, provide an IP address range (adjust based on your network)\nmicrok8s enable metallb:192.168.123.50-192.168.123.100\n</code></pre> <p>Learn more about MicroK8s add-ons.</p>"},{"location":"development/microk8s/#step-4-export-and-merge-kubeconfig","title":"Step 4: Export and Merge Kubeconfig","text":""},{"location":"development/microk8s/#41-export-kubeconfig-from-vms","title":"4.1 Export Kubeconfig from VMs","text":"<pre><code># On the management VM\nmicrok8s config &gt; ~/.kube/config\n\n# On the development VM\nmicrok8s config &gt; ~/.kube/config\n</code></pre>"},{"location":"development/microk8s/#42-copy-kubeconfig-files-to-host","title":"4.2 Copy Kubeconfig Files to Host","text":"<pre><code># From your host machine\nscp &lt;username&gt;@192.168.123.52:~/.kube/config ~/.kube/mgmt-config\nscp &lt;username&gt;@192.168.123.62:~/.kube/config ~/.kube/dev-config\n</code></pre>"},{"location":"development/microk8s/#43-edit-and-merge-kubeconfig-files","title":"4.3 Edit and Merge Kubeconfig Files","text":"<p>Edit each kubeconfig file to rename clusters and users:</p> <pre><code># Edit mgmt-kubeconfig.yaml to change:\n# - cluster name to \"microk8s-mgmt\" \n# - user name to \"admin-mgmt\"\n\n# Edit dev-kubeconfig.yaml to change:\n# - cluster name to \"microk8s-dev\"\n# - user name to \"admin-dev\"\n</code></pre> <p>Then merge them:</p> <pre><code>KUBECONFIG=~/.kube/config:~/.kube/mgmt-config:~/.kube/dev-config kubectl config view --flatten &gt; ~/.kube/merged-config\nmv ~/.kube/merged-config ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre>"},{"location":"development/microk8s/#step-5-setting-up-argocd","title":"Step 5: Setting Up ArgoCD","text":""},{"location":"development/microk8s/#51-install-argocd-on-management-cluster","title":"5.1 Install ArgoCD on Management Cluster","text":"<pre><code># Switch to management cluster context\nkubectl config use-context microk8s-mgmt\n\n# Create namespace\nkubectl create namespace argocd\n\n# Install ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Wait for all pods to be ready\nkubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s\n</code></pre>"},{"location":"development/microk8s/#52-access-argocd-ui","title":"5.2 Access ArgoCD UI","text":"<pre><code># Port-forward ArgoCD server (run this in a separate terminal)\nkubectl port-forward svc/argocd-server -n argocd 8080:443 --address 0.0.0.0\n\n# Get the initial admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Visit https://localhost:8080 to access the ArgoCD UI.</p>"},{"location":"development/microk8s/#53-configure-argocd-cli","title":"5.3 Configure ArgoCD CLI","text":"<pre><code># Login to ArgoCD from your host\nargocd login localhost:8080\n\n# Or if you've set up /etc/hosts entries:\nargocd login mgmt.k8tre.internal:8080\n</code></pre>"},{"location":"development/microk8s/#54-register-development-cluster-with-argocd","title":"5.4 Register Development Cluster with ArgoCD","text":"<pre><code># Add the development cluster to ArgoCD\nargocd cluster add microk8s-dev --name dev --label environment=dev\n</code></pre>"},{"location":"development/microk8s/#55-configure-argocd-for-kustomize-helm-overlays","title":"5.5 Configure ArgoCD for Kustomize Helm Overlays","text":"<p>Create and apply a ConfigMap patch:</p> <pre><code>cat &lt;&lt; EOF &gt; argocd-cm-patch.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  kustomize.buildOptions: \"--enable-helm --load-restrictor LoadRestrictionsNone\"\nEOF\n\nkubectl apply -f argocd-cm-patch.yaml\n</code></pre> <p>Restart the ArgoCD repo server to apply changes:</p> <pre><code>kubectl rollout restart deployment argocd-repo-server -n argocd\n</code></pre> <p>Learn more about ArgoCD configuration options.</p>"},{"location":"development/microk8s/#step-6-convenience-configurations","title":"Step 6: Convenience Configurations","text":""},{"location":"development/microk8s/#61-context-switching-aliases","title":"6.1 Context-Switching Aliases","text":"<p>Add to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code># For easy context switching\nalias kdev='kubectl config use-context microk8s-dev'\nalias kmgmt='kubectl config use-context microk8s-mgmt'\n\n# Optionally add kubectl aliases\nalias k='kubectl'\nalias kgp='kubectl get pods'\nalias kgs='kubectl get services'\n</code></pre> <p>Then source your file: <code>source ~/.bashrc</code></p>"},{"location":"development/microk8s/#62-host-file-entries","title":"6.2 Host File Entries","text":"<p>Add the following entries to <code>/etc/hosts</code> on your host machine:</p> <pre><code>sudo bash -c 'cat &lt;&lt; EOF &gt;&gt; /etc/hosts\n# VMs running MicroK8s\n192.168.123.52 mgmt.k8tre.internal\n192.168.123.62 dev.k8tre.internal\nEOF'\n</code></pre> <p>Note: Be careful not to assign IP addresses that might conflict with other devices on your network.</p>"},{"location":"development/microk8s/#step-7-verification","title":"Step 7: Verification","text":""},{"location":"development/microk8s/#71-test-cluster-access","title":"7.1 Test Cluster Access","text":"<pre><code># Test management cluster\nkmgmt\nkubectl get nodes\n\n# Test development cluster\nkdev\nkubectl get nodes\n</code></pre>"},{"location":"development/microk8s/#72-verify-argocd-setup","title":"7.2 Verify ArgoCD Setup","text":"<pre><code># Check that ArgoCD can communicate with both clusters\nargocd cluster list\n</code></pre>"},{"location":"development/microk8s/#additional-resources","title":"Additional Resources","text":"<ul> <li>MicroK8s Documentation</li> <li>Kubernetes Documentation</li> <li>ArgoCD Documentation</li> <li>Helm Documentation</li> <li>Kustomize Documentation</li> </ul>"},{"location":"development/microk8s/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Check cluster status: <code>microk8s status</code></li> <li>View MicroK8s logs: <code>microk8s inspect</code></li> <li>Check ArgoCD logs: <code>kubectl logs -n argocd deployment/argocd-server</code></li> <li>For networking issues, verify the VM network configuration.</li> </ol>"},{"location":"guides/configuration/","title":"Configuration Guide","text":""},{"location":"guides/installation/","title":"Installation Guide","text":"<p>AI Generated placeholder text - Not sanity checked!</p> <p>This guide explains how to install K8TRE.</p>"},{"location":"guides/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing, ensure you have:</p> <ul> <li>Kubernetes cluster v1.20+</li> <li>kubectl installed and configured</li> <li>Helm v3+ (optional)</li> </ul>"},{"location":"guides/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"guides/installation/#method-1-using-kubectl","title":"Method 1: Using kubectl","text":"<pre><code># Apply the manifest directly\nkubectl apply -f https://github.com/k8tre/k8tre/releases/latest/download/k8tre.yaml\n</code></pre>"},{"location":"guides/installation/#method-2-using-helm","title":"Method 2: Using Helm","text":"<pre><code># Add the Helm repository\nhelm repo add k8tre https://k8tre.github.io/k8tre/charts\n\n# Update repositories\nhelm repo update\n\n# Install the chart\nhelm install k8tre k8tre/k8tre\n</code></pre>"},{"location":"guides/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To verify that K8TRE is installed correctly:</p> <pre><code>kubectl get pods -n k8tre-system\n</code></pre> <p>You should see the K8TRE pods running.</p>"},{"location":"spec/","title":"Introduction","text":"<p>Each page in this folder contains a K8TRE Specification statement pertaining to a particular topic within the K8TRE Specification. They have been derived from k8tre GitHub Org Discussions, as well as discussion within the K8TRE working group, and reviewed by GitHub Pull Request for publication to this site.</p> <p>There a few important clarifications regarding the interpretation of these Specification statements:</p> <ol> <li> <p>Scope - The K8TRE Specification applies to Kubernetes-based TRE codebases. It is written for an audience of K8TRE codebase developers and defines what a K8TRE-compatible codebase must provide. This includes the K8TRE Reference Implementation as well as other codebases such as the UCL ARC TRE and the FRIDGE TRE.</p> </li> <li> <p>Technology Neutrality - Beyond its explicit reliance on Kubernetes, the specification is technology-agnostic. However, it may prescribe in detail the capabilities that must be provided by a codebase.</p> </li> <li> <p>Applicability \u2013 These statements are directed at those developing K8TRE-compliant codebases. They do not apply to administrators or operators running a TRE built on such a codebase (a \"deployment\").</p> </li> </ol>"},{"location":"spec/#design-principles","title":"Design Principles","text":"<ol> <li>The K8TRE Specification will define the capabilities that must be provided by the underlying Kubernetes platform.</li> <li>Whilst the Specification encourages Kubernetes best-practice, it will not impose particular implementation choices where many competing options exist. The Speicifcation should leave room for TRE developers to make independent implementation choices based on their own organisational/business requirements, whilst still allowing acceptable choices to comply with the Specification.</li> <li>Widespread compliance with the K8TRE Specification should result in different TRE developers in the TRE community being able to deploy each other's components in their own TRE's with minimal changes to code, only changes to configuration. In other words, increased portability of open-source TRE components and applications.</li> </ol>"},{"location":"spec/byosoftware/","title":"BYO Software","text":"<p>K8TRE Spec Statement: K8TRE supports both \"bring-your-own software and code\" and curated software models, but it should be up to the TRE administrators to determine what can be run.</p> <p>Last updated: 2025-07-07 Source: GitHub Discussion</p> <p>Questions:</p> <ol> <li> <p>What is K8TRE's stance on allowing researchers to ingress \"bring-your-own software and code\", versus a curated software model? Will it allow both?</p> <p>If it's software that runs inside the researcher's VM/workspace, it should be up to the TRE administrators to determine what can be run. If it's software that requires additional infrastructure, then this is a different question regarding compliant interfaces and prerequisites for arbitrary infrastructure interacting with a K8TRE instance.</p> </li> </ol>"},{"location":"spec/containerruntimes/","title":"Container Runtimes","text":"<p>K8TRE Spec Statement: All default container runtimes on AKS, EKS, K3S carry the risk of container breakout. For most TRE operators, this wouldn't be considered a significant risk. TRE operators who can not tolerate the risk of container breakouts should consider using a more secure lower level runtime such as Kata Containers or gVisor.</p> <p>Last updated: Unknown Source: GitHub Discussion</p> <p>Questions: </p> <ol> <li> <p>What container runtimes should a K8TRE implementation use, and why?</p> <p>A K8TRE implementation may use the default CRI- and OCI-compliant container runtimes of their chosen Kubernetes distribution, or where a TRE operator's risk appetite is low enough that the container breakout risk using these runtimes is unacceptable, more secure OCI-compliant runtimes may be used e.g. Kata Containers, gVisor.</p> </li> <li> <p>What statements about container runtimes must the K8TRE Specification make, pertaining to the capabilities that must be implemented by the underlying K8S platform?</p> <p>A K8TRE implementation's underlying Kubernetes platform needs to provide suitable project isolation for the TRE operator's risk appetite. The K8TRE Specification therefore needs to allow TRE operators to choose runtimes with increased security (allowing increased confidence in project isolation) should their preferences require it. However, the Specification must allow less risk averse operators to use the default CRI and OCI-compliant runtimes for EKS, AKS, and K3S.</p> </li> </ol>"},{"location":"spec/database/","title":"Databases","text":"<p>K8TRE Spec Statement: Databases should be treated as attached resources and may be deployed on the cluster, or apps may connect to off-cluster databases.</p> <p>Last updated: Unknown Source: GitHub Discussion</p> <p>Questions:</p> <ol> <li> <p>What should K8TRE Specification say about in-cluster DBs and what should it say about off-cluster DBs?</p> <p>Databases should be attached resources, explicitly referenced. TRE administrators may use an externally provided database service, such as AWS RDS, but where applications can use an on-cluster database, they should consider using the CNPG operator to deploy an instance of Postgres DB, rather than using a different Postgres helm chart which introduces an additional dependency.</p> </li> <li> <p>How prescriptive should the K8TRE Specification be in dictating how databases are deployed and managed on-cluster?</p> <p>The specification should remain non-prescriptive, but it ought to encourage modern best practices for database management, such as using Kubernetes-native tools like database operators (e.g. the CloudNativePG operator), to align with a decoupled, microservices-oriented architecture.</p> </li> </ol>"},{"location":"spec/dns/","title":"DNS","text":"<p>K8TRE Spec Statement: A TRE that deploys DNS records to allow external consumers to discover services should manage the external DNS entities together with the lifecycle operations of the corresponding services, such as deployments or upgrades. This includes removing DNS records which are no longer needed.</p> <p>Last updated: Unknown Source: GitHub Discussion</p> <p>Questions: </p> <ol> <li> <p>What will provide in-cluster DNS?</p> <p>The default CoreDNS will be suitable for most TRE implementers, since it allows access to services by servicename.namespace without a separate DNS server. A TRE implementer may use a different DNS implementation if it is necessary.</p> </li> </ol>"},{"location":"spec/gitops/","title":"GitOps","text":""},{"location":"spec/gitops/#gitops-using-argocd","title":"GitOps using ArgoCD","text":"<p>Questions: 1. What CI/CD tool is K8TRE going to use? 2. Should the tool deploy and manage applications to the same cluster it is installed on or deploy applications to other clusters? i.e. in-cluster vs cross-cluster architecture? 3. What are the limits of what the CI/CD tool manages?</p> <pre><code>1. Argo CD.\n2. Cross-cluster architecture supports two sub-models: \n    - deployment of K8TRE to dev/test/prod clusters from a single ArgoCD installation\n    - deployment of one K8TRE per project or deployment of ephemeral K8TRE development environments for each developer.\n3. In the JupyterHub control plane model, JupyterHub is responsible for creating/destroying workspaces, not ArgoCD. ArgoCD will complain that JupyterHub is out-of-sync because of the new resources but there are ways of addressing this.\n</code></pre>"},{"location":"spec/glossary/","title":"Glossary","text":"<p>This page is under construction.</p>"},{"location":"spec/loadbalancers/","title":"Load Balancers","text":""},{"location":"spec/loadbalancers/#load-balancers","title":"Load Balancers","text":"<p>Questions: </p> <ol> <li>Where should load balancers be used in the K8TRE Reference Implementation?</li> <li>Should a K8TRE be permitted to disaggregate load balancing from the ingress controller, so ought the K8TRE Specification leave the choice of which off-cluster load balancer up to implementers?</li> <li> <p>Where should they be mandatory/optional/recommended in the Specification?</p> <ol> <li>? </li> <li>? Yes - as long as there is one..?</li> <li>We should describe where load balancers are required in K8TRE.  </li> </ol> </li> </ol>"},{"location":"spec/networking/","title":"Networking","text":""},{"location":"spec/networking/#networking","title":"Networking","text":"<p>Questions: </p> <ol> <li>What capabilities must a CNI must provide the cluster to be K8TRE compliant?</li> <li>What CNI will K8TRE Reference Implementation use?</li> <li> <p>Should a K8TRE's CIDR be solely for in-cluster use only, or should applications/services outside the cluster also have access to the CIDR/VPC/VNET</p> <ol> <li>Full support for K8S Network Policies? i.e. not AWS VPC CNI or Azure CNI..?</li> <li>Cilium vs Calico - Cilium preferred, used in ARC TRE and FRIDGE</li> <li>No. A K8TRE's CIDR/VPC/VNET is solely for in-cluster use only so all external access is via the ingress/load-balancer</li> </ol> </li> </ol>"},{"location":"spec/secrets/","title":"Secrets","text":""},{"location":"spec/secrets/#secrets","title":"Secrets","text":"<p>Questions: 1. How do we store secrets in and make them available to applications on the cluster? Use k8s default secrets storage or more secure alternative backends? 2. How do we generate secrets and get them into k8s in the first place?</p> <pre><code>1. k8s default is to store secrets unencrypted in etcd, this is not acceptable. k8s offers you the options:\n    - encrypt at rest using a KMS provider and plugin to encrypt etcd. \n    - use the [secrets-store-csi-driver](https://secrets-store-csi-driver.sigs.k8s.io/concepts.html) and supported provider to access external secrets store.\n2. Use existing organisation secrets manager where possible, enabling centralised management of credentials across an org.\n</code></pre>"},{"location":"spec/storage/","title":"Storage","text":""},{"location":"spec/storage/#storage","title":"Storage","text":"<p>Questions: </p> <ol> <li>Which storage requirements shall the K8TRE Specification assume the underlying Kubernetes platform will provide? e.g. what storageClass definitions / providers should be recommended/mandated?</li> <li> <p>Which Persistent Volume Types/plugins will K8TRE Reference Implementation use?</p> <ol> <li>Storage classes should be defined for any K8TRE to use.</li> <li>AWS = , Azure = , K3S = </li> </ol> </li> </ol>"}]}